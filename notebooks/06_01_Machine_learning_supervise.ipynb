{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 6 - Différentes utilisations dumachine learning avec Python (1ère partie)\n",
    "\n",
    "## 6. 4 L’apprentissage supervisé avec Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les méthodes d’apprentissage supervisé sont les méthodes actuellement les plus\n",
    "utilisées en data science. Il s’agit d’essayer de prédire une variable cible et d’utiliser\n",
    "différentes méthodes pour arriver à cette fin.\n",
    "Nous allons illustrer ces méthodes de traitement de données avec du code et des\n",
    "cas pratiques.\n",
    "### 6.4.1 Les données et leur transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce jeu de données est décrit en détail au début du chapitre 4. Pour rappel, il est composé de 3333 individus et de 18 variables. Il est stocké dans un fichier csv, nommé telecom.csv, accessible dans le répertoire Data. On le récupère en utilisant Pandas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn=pd.read_csv(\"../data/telecom.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce jeu de données n’a pas de données manquantes et nous allons devoir effectuer\n",
    "quelques transformations pour l’adapter à nos traitements. Nous voyons par exemple qu’il est composé de trois colonnes object.\n",
    "\n",
    "Nous pouvons afficher les statistiques descriptives pour les colonnes object :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>3333</td>\n",
       "      <td>51</td>\n",
       "      <td>WV</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phone</th>\n",
       "      <td>3333</td>\n",
       "      <td>3333</td>\n",
       "      <td>382-4657</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Int'l Plan</th>\n",
       "      <td>3333</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>3010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VMail Plan</th>\n",
       "      <td>3333</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>2411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Churn?</th>\n",
       "      <td>3333</td>\n",
       "      <td>2</td>\n",
       "      <td>False.</td>\n",
       "      <td>2850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count unique       top  freq\n",
       "State       3333     51        WV   106\n",
       "Phone       3333   3333  382-4657     1\n",
       "Int'l Plan  3333      2        no  3010\n",
       "VMail Plan  3333      2        no  2411\n",
       "Churn?      3333      2    False.  2850"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn.describe(include=\"object\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que les données sont toutes binaires. \n",
    "\n",
    "Pour les variables binaires, il nous suffit de les recoder avec Scikit-Learn pour obtenir des données exploitables. Par\n",
    "ailleurs, il existe une autre variable qualitative dans notre jeu de données, Area Code,\n",
    "qui est numérique mais avec trois modalités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415    1655\n",
       "510     840\n",
       "408     838\n",
       "Name: Area Code, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn[\"Area Code\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La préparation des données\n",
    "\n",
    "Nous allons utiliser le processus de traitement classique pour transformer nos\n",
    "données avec Scikit-Learn. Dans ce cas, nous n’avons pas de données manquantes,\n",
    "nous travaillons donc sur la transformation des variables qualitatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabelEncoder va nous permettre de transformer les valeurs textuelles en entiers.\n",
    "Nous pouvons utiliser pour chaque variable qualitative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "churn[\"Churn?\"] = encoder.fit_transform(churn[\"Churn?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label_encode={}\n",
    "for col in churn.columns:\n",
    "    if churn[col].dtype == object:\n",
    "        dict_label_encode[col]=LabelEncoder()\n",
    "        churn[col]=dict_label_encode[col].fit_transform(churn[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons la classe OneHotEncoder afin de transformer la colonne Area Code\n",
    "en trois colonnes binaires (le chapitre 4 détaille le fonctionnement de cet outil) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définit l’objet\n",
    "encoder_area=OneHotEncoder(sparse=False,categories=\"auto\")\n",
    "\n",
    "# on crée un array dans lequel on stocke le résultat\n",
    "area=encoder_area.fit_transform(np.array(churn[\"Area Code\"]).reshape(-1, 1))\n",
    "\n",
    "# on définit un DataFrame nettoyé qui consiste en la concaténation du\n",
    "# DataFrame initial et des nouvelles colonnes\n",
    "churn_clean=pd.concat([churn, pd.DataFrame(area,columns=encoder_area.categories_,\n",
    "                                           index=churn.index)], axis=1)\n",
    "# on supprime la colonne initiale\n",
    "churn_clean.drop(\"Area Code\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Account Length</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Int'l Plan</th>\n",
       "      <th>VMail Plan</th>\n",
       "      <th>VMail Message</th>\n",
       "      <th>Day Mins</th>\n",
       "      <th>Day Calls</th>\n",
       "      <th>Day Charge</th>\n",
       "      <th>Eve Mins</th>\n",
       "      <th>...</th>\n",
       "      <th>Night Calls</th>\n",
       "      <th>Night Charge</th>\n",
       "      <th>Intl Mins</th>\n",
       "      <th>Intl Calls</th>\n",
       "      <th>Intl Charge</th>\n",
       "      <th>CustServ Calls</th>\n",
       "      <th>Churn?</th>\n",
       "      <th>(408,)</th>\n",
       "      <th>(415,)</th>\n",
       "      <th>(510,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>1926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>107</td>\n",
       "      <td>1575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>137</td>\n",
       "      <td>1117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>...</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>84</td>\n",
       "      <td>1707</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>75</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>3</td>\n",
       "      <td>192</td>\n",
       "      <td>3057</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>156.2</td>\n",
       "      <td>77</td>\n",
       "      <td>26.55</td>\n",
       "      <td>215.5</td>\n",
       "      <td>...</td>\n",
       "      <td>83</td>\n",
       "      <td>12.56</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>49</td>\n",
       "      <td>68</td>\n",
       "      <td>1528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>231.1</td>\n",
       "      <td>57</td>\n",
       "      <td>39.29</td>\n",
       "      <td>153.4</td>\n",
       "      <td>...</td>\n",
       "      <td>123</td>\n",
       "      <td>8.61</td>\n",
       "      <td>9.6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.8</td>\n",
       "      <td>109</td>\n",
       "      <td>30.74</td>\n",
       "      <td>288.8</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>8.64</td>\n",
       "      <td>14.1</td>\n",
       "      <td>6</td>\n",
       "      <td>3.81</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>6</td>\n",
       "      <td>184</td>\n",
       "      <td>1331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213.8</td>\n",
       "      <td>105</td>\n",
       "      <td>36.35</td>\n",
       "      <td>159.6</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>42</td>\n",
       "      <td>74</td>\n",
       "      <td>2559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>234.4</td>\n",
       "      <td>113</td>\n",
       "      <td>39.85</td>\n",
       "      <td>265.9</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>10.86</td>\n",
       "      <td>13.7</td>\n",
       "      <td>4</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3333 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      State  Account Length  Phone  Int'l Plan  VMail Plan  VMail Message  \\\n",
       "0        16             128   1926           0           1             25   \n",
       "1        35             107   1575           0           1             26   \n",
       "2        31             137   1117           0           0              0   \n",
       "3        35              84   1707           1           0              0   \n",
       "4        36              75    110           1           0              0   \n",
       "...     ...             ...    ...         ...         ...            ...   \n",
       "3328      3             192   3057           0           1             36   \n",
       "3329     49              68   1528           0           0              0   \n",
       "3330     39              28     42           0           0              0   \n",
       "3331      6             184   1331           1           0              0   \n",
       "3332     42              74   2559           0           1             25   \n",
       "\n",
       "      Day Mins  Day Calls  Day Charge  Eve Mins  ...  Night Calls  \\\n",
       "0        265.1        110       45.07     197.4  ...           91   \n",
       "1        161.6        123       27.47     195.5  ...          103   \n",
       "2        243.4        114       41.38     121.2  ...          104   \n",
       "3        299.4         71       50.90      61.9  ...           89   \n",
       "4        166.7        113       28.34     148.3  ...          121   \n",
       "...        ...        ...         ...       ...  ...          ...   \n",
       "3328     156.2         77       26.55     215.5  ...           83   \n",
       "3329     231.1         57       39.29     153.4  ...          123   \n",
       "3330     180.8        109       30.74     288.8  ...           91   \n",
       "3331     213.8        105       36.35     159.6  ...          137   \n",
       "3332     234.4        113       39.85     265.9  ...           77   \n",
       "\n",
       "      Night Charge  Intl Mins  Intl Calls  Intl Charge  CustServ Calls  \\\n",
       "0            11.01       10.0           3         2.70               1   \n",
       "1            11.45       13.7           3         3.70               1   \n",
       "2             7.32       12.2           5         3.29               0   \n",
       "3             8.86        6.6           7         1.78               2   \n",
       "4             8.41       10.1           3         2.73               3   \n",
       "...            ...        ...         ...          ...             ...   \n",
       "3328         12.56        9.9           6         2.67               2   \n",
       "3329          8.61        9.6           4         2.59               3   \n",
       "3330          8.64       14.1           6         3.81               2   \n",
       "3331          6.26        5.0          10         1.35               2   \n",
       "3332         10.86       13.7           4         3.70               0   \n",
       "\n",
       "      Churn?  (408,)  (415,)  (510,)  \n",
       "0          0     0.0     1.0     0.0  \n",
       "1          0     0.0     1.0     0.0  \n",
       "2          0     0.0     1.0     0.0  \n",
       "3          0     1.0     0.0     0.0  \n",
       "4          0     0.0     1.0     0.0  \n",
       "...      ...     ...     ...     ...  \n",
       "3328       0     0.0     1.0     0.0  \n",
       "3329       0     0.0     1.0     0.0  \n",
       "3330       0     0.0     0.0     1.0  \n",
       "3331       0     0.0     0.0     1.0  \n",
       "3332       0     0.0     1.0     0.0  \n",
       "\n",
       "[3333 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous stockons d'abord le nom des colonnes quantitatives\n",
    "numeric_features = [ 'VMail Message','Day Mins', \n",
    "                    'Day Calls', 'Day Charge',\n",
    "                    'Eve Mins', 'Eve Calls', \n",
    "                    'Eve Charge', 'Night Mins', \n",
    "                    'Night Calls','Night Charge', \n",
    "                    'Intl Mins', 'Intl Calls',\n",
    "                    'Intl Charge','CustServ Calls']\n",
    "# nous appliquons à ces colonnes une transformation\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# nous stockons d'abord le nom des colonnes qualitatives\n",
    "categorical_features = ['Area Code', \n",
    "                        \"Int'l Plan\",\n",
    "                        'VMail Plan']\n",
    "# nous appliquons à ces colonnes une OneHot\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore',\n",
    "                                        categories='auto')\n",
    "\n",
    "# on combine toutes ces informations dans un objet de la\n",
    "# classe ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on applique les transformations sur toutes les colonnes\n",
    "churn_clean = preprocessor.fit_transform(churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédire l’attrition des clients\n",
    "Lorsqu’on veut prédire une variable binaire, on devra avoir une colonne du type\n",
    "binaire. On préfère généralement un codage 0/1 afin de garder un type entier simple à gérer. \n",
    "\n",
    "Les variables explicatives x auront été préparées de manière intelligente afin de bien appliquer nos modèles.\n",
    "\n",
    "On crée donc x et y :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = churn_clean\n",
    "y = churn[\"Churn?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.name=\"Churn!\"\n",
    "pd.DataFrame(x).to_csv(\"../data/x_telecom.csv\")\n",
    "y.to_csv(\"../data/y_telecom.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la séparation, on utilise la fonction train_test_split() de Scikit-Learn.\n",
    "\n",
    "Cette fonction permet de créer automatiquement autant de structures que nécessaire\n",
    "à partir de nos données. \n",
    "\n",
    "Elle utilise une randomisation des individus et ensuite une séparation en fonction d’un paramètre du type test_size :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on importe la fonction\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dans ce cas on a :\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans certains cas, il peut arriver qu’il y ait une forte disparité de distribution des\n",
    "modalités entre les proportions d’acceptation et de refus. On peut vouloir faire en\n",
    "sorte que les répartitions des modalités de y soient égales dans les différents échantillons,\n",
    "on pourra alors utiliser une stratification. On va utiliser une stratification en\n",
    "prenant y comme base pour effectuer la stratification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.855086\n",
       "1    0.144914\n",
       "Name: Churn!, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify = y)\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi les deux échantillons _train et _test ont la même distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.855\n",
       "1    0.145\n",
       "Name: Churn!, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.855122\n",
       "1    0.144878\n",
       "Name: Churn!, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les données déséquilibrées\n",
    "Ce problème se pose souvent lorsque vous cherchez à identifier des évènements\n",
    "rares, notamment la recherche de fraudes. Dans ce cas, la variable cible y a souvent\n",
    "moins de 1 % d’une des deux modalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearMiss\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_echant = NearMiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on applique la méthode fit_resample() sur nos données\n",
    "x_sous, y_sous= ss_echant.fit_resample(x_train, y_train)\n",
    "# on le transforme en Series pour avoir un affichage psl uagréable\n",
    "pd.Series(y_sous).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit donc que le nombre de 1 (classe minoritaire) n’a pas changé.\n",
    "Cette étape permet d’obtenir des données x construites de manière à bien différencier\n",
    "les individus.\n",
    "Pour le sur-échantillonnage, l’algorithme SMOTE (Synthetic Minority Oversampling\n",
    "Technique) fait référence. Cet algorithme va permettre de créer des individus proches\n",
    "de ceux de la classe minoritaire afin de sur-représenter cette classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sur_echant=SMOTE()\n",
    "# on applique la méthode fit_resample() sur nos données\n",
    "x_sur, y_sur= sur_echant.fit_resample(x_train, y_train)\n",
    "# on le transforme en Series pour avoir un affichage psl uagréable\n",
    "pd.Series(y_sur).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici que le nombre d’individus de la classe majoritaire reste le même.\n",
    "Ce package propose aussi une approche intermédiaire qui va combiner du souséchantillonnage\n",
    "et du sur-échantillonnage, et qui s’adapte bien, soit lorsque le jeu\n",
    "de données global est très grand, soit quand la classe sous-représentée a très peu\n",
    "d’occurrences. On utilise une combinaison de SMOTE et de plus proches voisins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "comb_echant=SMOTEENN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on applique la méthode fit_resample() sur nos données\n",
    "x_comb, y_comb= comb_echant.fit_resample(x_train, y_train)\n",
    "# on le transforme en Series pour avoir un affichage plus agréable\n",
    "pd.Series(y_comb).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu’on arrive à un jeu de données intermédiaire.\n",
    "L’approche à privilégier dépendra de vos données. Dans le cadre de notre exemple,\n",
    "nous allons garder la répartition actuelle afin d’illustrer la suite.\n",
    "Nous avons donc quatre jeux de données prêts à être utilisés pour nos traitements.\n",
    "L’étape suivante est le choix de l’algorithme de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 Le choix et l’ajustement de l’algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on crée un objet à partir de la classe du modèle en lui fournissant les\n",
    "hyperparamètres dont il a besoin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_rf=RandomForestClassifier()\n",
    "modele_knn=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas, on prend les hyperparamètres par défaut.\n",
    "\n",
    "On peut ensuite ajuster notre modèle en utilisant les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_rf.fit(x_train,y_train)\n",
    "modele_knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois qu’on a estimé les paramètres du modèle, on va pouvoir extraire des\n",
    "informations. De nouveaux attributs de chaque classe apparaissent, ils se terminent par le symbole underscore _ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui va nous intéresse avant tout, c’est de prédire avec notre modèle. Pour cela nous allons utiliser la méthode .predict() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_rf = modele_rf.predict(x_test)\n",
    "y_predict_knn = modele_knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient ainsi une valeur prédite pour les éléments de notre échantillon de\n",
    "validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas d’un modèle de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur les données des territoires d’Ile-de-France, nous désirons prédire le salaire\n",
    "médian des communes en fonction des autres variables des données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../data/base-comparateur-de-territoires.csv\",sep=\";\").dropna()\n",
    "# on prépare les données\n",
    "y_quanti=data[\"MED14\"]\n",
    "x_quanti=data.select_dtypes(np.number).drop([\"CODGEO\",\"REG\",\"DEP\",\"MED14\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sépare les échantillons apprentissage\n",
    "x_train_cont, x_test_cont, y_train_cont, y_test_cont = train_test_split(x_quanti,\n",
    "                                                                        y_quanti,\n",
    "                                                                        test_size=0.3)\n",
    "\n",
    "# on utilise un modèle de régression de Ridge\n",
    "modele_ridge = Ridge()\n",
    "\n",
    "# on ajuste le modèle aux données\n",
    "modele_ridge.fit(x_train_cont, y_train_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut afficher les coefficients du modèle :\n",
    "modele_ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on prédit grâce à ce modèle\n",
    "y_predict_cont=modele_ridge.predict(x_test_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 Les indicateurs pour valider un modèle\n",
    "La partie validation d’un modèle d’apprentissage supervisé est extrêmement\n",
    "importante. L’objectif d’un modèle d’apprentissage supervisé est de prédire une\n",
    "valeur la plus proche possible de la réalité. Nous différencions trois types d’indices\n",
    "en fonction du type de variable cible. Tous les indicateurs de qualité du modèle sont\n",
    "stockés dans le module *metrics* de Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le pourcentage de bien classés\n",
    "Il s’agit de l’indicateur le plus connu. On le nomme accuracy. Il est calculé à partir du rapport entre le nombre d’individus bien classés et le nombre total d’individus dans l’échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_modele_rf=accuracy_score(y_test,y_predict_rf)\n",
    "accuracy_modele_knn=accuracy_score(y_test,y_predict_knn)\n",
    "print(\"Pourcentage de bien classés pour le modèle RF :\",accuracy_modele_rf)\n",
    "print(\"Pourcentage de bien classés pour le modèle kNN :\",accuracy_modele_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La matrice de confusion\n",
    "Il s’agit d’un autre indicateur important pour juger de la qualité d’un modèle, il n’est pas défini par une seule valeur mais par une matrice dans laquelle on peut lire le croisement entre les valeurs observées et les valeurs prédites à partir du modèle. \n",
    "\n",
    "Pour calculer cette matrice, on pourra utiliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_rf=confusion_matrix(y_test,y_predict_rf)\n",
    "confusion_matrix_knn=confusion_matrix(y_test,y_predict_knn)\n",
    "print(\"Matrice de confusion pour le modèle RF :\",\n",
    "confusion_matrix_rf, sep=\"\\n\")\n",
    "print(\"Matrice de confusion pour le modèle kNN :\",\n",
    "confusion_matrix_knn, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le rappel (recall), la précision et le f1-score\n",
    "\n",
    "Scikit-Learn possède des fonctions pour chacun de ces indicateurs, mais il peut\n",
    "être intéressant d’utiliser une autre fonction qui les affiche pour chaque classe :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Rapport pour le modèle RF :\",\n",
    "      classification_report(y_test,y_predict_rf) ,sep=\"\\n\")\n",
    "print(\"Rapport pour le modèle kNN :\",\n",
    "      classification_report(y_test,y_predict_rf) ,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La courbe ROC\n",
    "Nous avons vu que lorsque les classes sont fortement déséquilibrées, la matrice\n",
    "de confusion est parfois dure à interpréter. La courbe ROC (Receiver Operating\n",
    "Characteristic) est là pour combler ce défaut. Elle est en fait la proportion de vrais positifs en fonction de la proportion de faux positifs.\n",
    "\n",
    "Il s’agit en fait de représenter le rappel (recall) en fonction de (1- spécificité) sur une courbe en faisant varier le seuil de classification (c’est-à-dire le point à partir duquel une observation est considérée comme positive).\n",
    "\n",
    "La courbe ROC s’obtient avec Scikit-Learn et Matplotlib :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# on extrait les probabilités d'appartenance\n",
    "proba_rf= modele_rf.predict_proba(x_test)[:,1]\n",
    "proba_knn=modele_knn.predict_proba(x_test)[:,1]\n",
    "\n",
    "# cas du modèle RF\n",
    "fpr, tpr, _ = roc_curve(y_test, proba_rf)\n",
    "plt.plot(fpr,tpr,\"b-.\", label=\"RF\")\n",
    "\n",
    "# cas du modèle kNN\n",
    "fpr, tpr, _ = roc_curve(y_test, proba_knn)\n",
    "plt.plot(fpr,tpr,\":\", label=\"kNN\")\n",
    "\n",
    "# modèle aléatoire\n",
    "plt.plot([0, 1], [0, 1],\"r-\", label=\"aléatoire\", )\n",
    "\n",
    "# modèle parfait\n",
    "plt.plot([0,0, 1], [0,1, 1], 'b--', label=\"parfait\")\n",
    "\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L’aire sous la courbe ROC\n",
    "La courbe ROC est un indicateur important mais on préfère souvent une valeur plutôt\n",
    "qu’une courbe afin de comparer nos modèles. Pour cela, on utilise l’aire sous la courbe\n",
    "ROC (AUC). Cette aire est calculée directement à partir de la courbe ROC. Ainsi, un\n",
    "modèle aléatoire aura une AUC de 0.5 et un modèle parfait aura une AUC de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_modele_rf=roc_auc_score(y_test, modele_rf.predict_proba(x_test)[:,1])\n",
    "auc_modele_knn=roc_auc_score(y_test,modele_knn.predict_proba(x_test)[:,1])\n",
    "\n",
    "print(\"Aire sous la courbe ROC pour le modèle RF :\" ,auc_modele_rf)\n",
    "print(\"Aire sous la courbe ROC pour le modèle kNN :\" ,auc_modele_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La validation croisée\n",
    "Jusqu’ici nous avons utilisé des indicateurs basés sur une seule occurrence de test. Ceci veut dire qu’on ne teste notre modèle que sur un seul échantillon.\n",
    "\n",
    "Une approche alternative souvent utilisée est la validation croisée. Celle-ci est en fait basée sur la répétition de l’estimation et de la validation sur des données différentes.\n",
    "\n",
    "Pour obtenir ce cv-score, on utilise :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_rf = cross_val_score(modele_rf, x, y, cv=5, scoring='roc_auc')\n",
    "scores_knn = cross_val_score(modele_knn, x, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"AUC pour RF : %.2f (+/- %.2f)\"% (scores_rf.mean(), scores_knn.std() * 2))\n",
    "print(\"AUC pour kNN : %.2f (+/- %.2f)\"% (scores_knn.mean(),scores_knn.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La validation dans le cas continu\n",
    "\n",
    "Il s’agit du cas où on essaye de prédire une variable quantitative. Dans ce cas, on ne peut pas utiliser les indicateurs précédents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "# le R², à la différence des autres scores, est généralmeent\n",
    "# calculé sur l’échantillon d’apprentissage\n",
    "print(\"R2 :\",r2_score(y_train_cont,modele_ridge.predict(x_train_cont)))\n",
    "\n",
    "# les autres indicateurs sont basés sur l’échantillon de validation\n",
    "print(\"MCE :\",mean_squared_error(y_test_cont,y_predict_cont))\n",
    "print(\"RMCE :\",np.sqrt(mean_squared_error(y_test_cont,y_predict_cont)))\n",
    "print(\"MAE :\",median_absolute_error(y_test_cont,y_predict_cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.4 L’ajustement des hyperparamètres d’un modèle\n",
    "\n",
    "L’une des tâches du data scientist est de trouver le meilleur modèle possible. La\n",
    "plupart des modèles de machine learning ont des hyperparamètres. Il s’agit de paramètres\n",
    "du modèle qui sont définis en amont de l’ajustement.\n",
    "\n",
    "Scikit-Learn propose une classe GridSearchCV permettant d’implémenter cette\n",
    "recherche d’hyperparamètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va donc devoir définir les hyperparamètres que l’on souhaite tester. Pour cela,\n",
    "on utilisera un dictionnaire d’hyperparamètres, par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_param= {\"max_depth\":[3,5,7,10], \"n_estimators\":[10,20,50,100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va encore utiliser l’accuracy pour valider notre modèle. Finalement, nous allons\n",
    "utiliser une validation croisée à cinq groupes pour valider les résultats.\n",
    "Le nouvel objet est le suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recherche_hyper = GridSearchCV(RandomForestClassifier(), \n",
    "                               dico_param, \n",
    "                               scoring=\"accuracy\",cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois qu’on a créé cet objet, on peut lui joindre les données afin d’estimer les\n",
    "meilleurs paramètres du modèle.\n",
    "\n",
    "Cette étape peut être très longue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recherche_hyper.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous revenons à notre exemple de données des télécommunications avec nos\n",
    "deux estimateurs par forêt aléatoire et par plus proches voisins. Nous allons faire\n",
    "varier les hyperparamètres de ces deux modèles pour trouver la meilleure combinaison\n",
    "en termes d’aire sous la courbe ROC :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# construction des dictionnaires d’hyperparamètres\n",
    "dico_param_rf={\"n_estimators\":[10,100,1000],\"max_depth\":[5,7,9]}\n",
    "dico_param_knn={\"n_neighbors\":[2,5,10,50],\"weights\":['uniform','distance']}\n",
    "\n",
    "modele_grid_rf=GridSearchCV(modele_rf,dico_param_rf,\n",
    "                            scoring=\"roc_auc\",cv=5)\n",
    "\n",
    "modele_grid_knn=GridSearchCV(modele_knn, dico_param_knn,\n",
    "                             scoring=\"roc_auc\",cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation des paramètres et des meilleurs modèles\n",
    "modele_grid_rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation des paramètres et des meilleurs modèles\n",
    "modele_grid_knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des AUC pour la combinaison obtenant\n",
    "# les meilleurs résultats\n",
    "print(\"Meilleurs paramètres RF:\", modele_grid_rf.best_params_)\n",
    "print(\"AUC - RF:\", modele_grid_rf.best_score_)\n",
    "print(\"Meilleurs paramètres kNN:\", modele_grid_knn.best_params_)\n",
    "print(\"AUC - kNN:\", modele_grid_knn.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que malgré ce travail, les forêts aléatoires sont toujours plus efficaces que\n",
    "les plus proches voisins.\n",
    "Si le nombre d’hyperparamètres devient trop grand, on utilise alors une recherche\n",
    "aléatoire parmi les combinaisons avec la classe RandomSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.5 La construction d’un pipeline de traitement\n",
    "\n",
    "Bien souvent vous allez être amené à enchaîner des traitements sur des données.\n",
    "On peut bien sûr développer son code de manière à suivre les étapes une à une mais il est souvent plus intéressant de créer des suites de traitements automatisées avec Scikit-Learn. \n",
    "\n",
    "Ces suites de traitements sont appelées pipeline. Ils simplifieront votre code et permettront de passer en production simplement.\n",
    "\n",
    "Ainsi, on va pouvoir faire une analyse en composantes principales suivies d’un\n",
    "algorithme de plus proches voisins directement dans un pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp=PCA(n_components=8)\n",
    "knn=KNeighborsClassifier()\n",
    "pipe=Pipeline(steps=[(\"acp\",acp),(\"knn\",knn)])\n",
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,pipe.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ainsi enchaîné deux traitements. Si on cherche des sorties liées à chacune\n",
    "des étapes, on pourra le faire simplement. Par exemple, si l’objectif est d’extraire la part de variances expliquées par les composantes de l’analyse en composantes principales, on fera :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps[\"acp\"].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trouver la meilleure combinaison d’hyperparamètres dans un pipeline\n",
    "\n",
    "Essayons de trouver la meilleure combinaison d’hyperparamètres dans un pipeline.\n",
    "Dans le cadre de cet exemple, nous utiliserons les SVM (support vector machines,\n",
    "également appelés séparateurs à vaste marge ou machines à vecteurs de support).\n",
    "Ce sont des méthodes assez complexes dans leur principe mais simples dans leur\n",
    "mise en oeuvre.\n",
    "\n",
    "Les hyperparamètres d’un modèle de SVM sont assez nombreux. Les plus importants\n",
    "étant le noyau choisi (linéaire, polynomial, sigmoïd, RBF…), les paramètres de\n",
    "ces noyaux (le degré pour le cas polynomiale, gamma…) et le C pour la marge floue.\n",
    "\n",
    "Dans notre exemple, on utilisera la fonction make_pipeline qui est équivalente\n",
    "à la classe précédente avec quelques simplifications :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# construction du pipeline basé sur deux approches\n",
    "mon_pipe=make_pipeline(PCA(), SVC(probability=True))\n",
    "\n",
    "# construction du dictionnaire des paramètres\n",
    "# (attention utilisation de __)\n",
    "param_grid = dict(pca__n_components=[5, 10, x_train.shape[1]],\n",
    "                  svc__C= [1, 10, 100, 1000],\n",
    "                  svc__kernel= ['sigmoid', 'rbf'],\n",
    "                  svc__gamma= [0.001, 0.0001])\n",
    "\n",
    "# on construit l’objet GridSearch et on estime les hyper-paramètres\n",
    "# par validation croisée\n",
    "grid_search_mon_pipe = GridSearchCV(mon_pipe, param_grid = param_grid, scoring = \"roc_auc\", cv = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_mon_pipe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la meilleure combinaisons de paramètres est :\n",
    "grid_search_mon_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les meilleurs hyperparamètres obtenus en utilisant l’aire sous la courbe ROC sont\n",
    "la combinaison C = 10, gamma = 0.0001, un noyau RBF pour les SVM et dix composantes\n",
    "pour notre analyse en composantes principales.\n",
    "\n",
    "Dans ce code, on définit les hyperparamètres associés à une méthode du pipeline\n",
    "avec un double underscore : __.\n",
    "\n",
    "L’utilisation des pipelines de Scikit-Learn va devenir rapidement une étape cruciale\n",
    "de vos développements en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.6 Passer en production votre modèle d’apprentissage supervisé\n",
    "\n",
    "#### Persistance de modèle avec Scikit-Learn\n",
    "\n",
    "Python possède plusieurs outils pour la persistance d’objets, c’est-à-dire pour stocker\n",
    "des objets dans des fichiers. Les objets de Scikit-Learn sont aussi dans cette\n",
    "situation. On utilise un format pickle qui aura l’extension .pkl.\n",
    "\n",
    "Par exemple, si nous voulons sauvegarder le dernier pipeline de traitement, nous\n",
    "allons utiliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(grid_search_mon_pipe, '../data/modele_grid_pipe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois ce modèle stocké, on peut très bien le réutiliser dans un autre cadre. Si\n",
    "nous créons un nouveau notebook, nous allons utiliser :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "grid_search_mon_pipe = joblib.load('../data/modele_grid_pipe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite appliquer le modèle avec tous les paramètres qui ont été appris :\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grid_search_mon_pipe.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’utilisation d’un fichier Pickle dans un notebook est une technique assez simple et courante."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
